{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfe28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 1: Imports and Setup (Updated)\n",
    "import pygame\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque # Efficient queue for replay buffer\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Pygame Initialization ---\n",
    "# Keep this even if not rendering during training, needed for evaluation cell later\n",
    "try:\n",
    "    pygame.init()\n",
    "    pygame.font.init()\n",
    "    FONT = pygame.font.SysFont(\"Arial\", 20)\n",
    "    print(\"Pygame initialized successfully.\")\n",
    "    PYGAME_INITIALIZED = True\n",
    "except pygame.error as e:\n",
    "    print(f\"Pygame initialization failed: {e}\")\n",
    "    PYGAME_INITIALIZED = False\n",
    "    FONT = None\n",
    "\n",
    "# --- Device Selection (GPU/CPU) ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "# --- Configuration / Hyperparameters ---\n",
    "# Simulation\n",
    "WIDTH, HEIGHT = 1000, 700\n",
    "TRACK_COLOR = (100, 100, 100); GRASS_COLOR = (0, 150, 0)\n",
    "CAR_COLOR = (255, 0, 0); SENSOR_COLOR = (0, 255, 255)\n",
    "CHECKPOINT_COLOR = (255, 255, 0, 100); OBSTACLE_COLOR = (0, 0, 255)\n",
    "\n",
    "CAR_WIDTH, CAR_HEIGHT = 15, 30\n",
    "MAX_SPEED = 7; MIN_SPEED = 0\n",
    "ACCELERATION = 0.15; BRAKE_DECEL = 0.3; FRICTION = 0.04\n",
    "STEERING_ANGLE = 10\n",
    "NUM_SENSORS = 9; SENSOR_RANGE = 180\n",
    "\n",
    "# RL Agent (DQN)\n",
    "STATE_SIZE = NUM_SENSORS + 1\n",
    "ACTION_SIZE = 4\n",
    "HIDDEN_SIZE = 256\n",
    "LEARNING_RATE = 0.00025 # Slightly lower learning rate\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "# ***********************************************************\n",
    "# ***** SLOWER EPSILON DECAY *****\n",
    "# ***********************************************************\n",
    "# Decay over ~800-900 episodes: 0.01 = 1.0 * (decay^X) => log(0.01) = X*log(decay) => X = log(0.01)/log(decay)\n",
    "# If X = 800000 steps (e.g. 1000 eps * 800 steps/ep), decay = exp(log(0.01)/800000) approx 0.999994\n",
    "# Let's try decaying over ~500k steps (might be ~600-700 episodes avg)\n",
    "EPSILON_DECAY = 0.99999 # Much slower decay per training step\n",
    "# ***********************************************************\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 10 # Update target network every N episodes\n",
    "\n",
    "# Training\n",
    "NUM_EPISODES = 1000 # Keep 1000, might need more\n",
    "MAX_STEPS_PER_EPISODE = 2000\n",
    "\n",
    "# Visualization & Saving\n",
    "# ***********************************************************\n",
    "# ***** DISABLE RENDERING DURING TRAINING *****\n",
    "# ***********************************************************\n",
    "RENDER_EVERY_N_EPISODES = 0 # Set to 0 to disable rendering during training loop\n",
    "# ***********************************************************\n",
    "PLOT_UPDATE_FREQ = 50\n",
    "MODEL_SAVE_FREQ = 50\n",
    "MODEL_SAVE_DIR = \"dqn_car_models_complex_duel\" # New directory\n",
    "PLOT_SAVE_DIR = \"dqn_car_plots_complex_duel\"   # New directory\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"State Size: {STATE_SIZE}, Action Size: {ACTION_SIZE}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Rendering during training: {'Enabled' if RENDER_EVERY_N_EPISODES > 0 else 'DISABLED'}\")\n",
    "print(f\"Epsilon decay rate: {EPSILON_DECAY} (per training step)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65520456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 2: Track Definition (Unchanged)\n",
    "\n",
    "# Define a more complex track with curves\n",
    "OUTER_TRACK_POINTS = [\n",
    "    (50, 150), (150, 50), (400, 50), (550, 100), (700, 100), (850, 50),\n",
    "    (950, 150), (950, 550), (850, 650), (700, 600), (550, 600), (400, 650),\n",
    "    (150, 650), (50, 550)\n",
    "]\n",
    "\n",
    "INNER_TRACK_POINTS = [\n",
    "    (150, 250), (250, 150), (400, 150), (500, 200), (650, 200), (750, 150),\n",
    "    (850, 250), (850, 450), (750, 550), (650, 500), (500, 500), (400, 550),\n",
    "    (250, 550), (150, 450)\n",
    "]\n",
    "\n",
    "# Define Static Obstacles (as pygame.Rect objects)\n",
    "OBSTACLES = [\n",
    "    pygame.Rect(300, 300, 20, 100), # Vertical obstacle\n",
    "    pygame.Rect(600, 400, 100, 20), # Horizontal obstacle\n",
    "    pygame.Rect(700, 200, 50, 50)   # Square obstacle near a turn\n",
    "]\n",
    "\n",
    "# Define Checkpoints for the new track\n",
    "CHECKPOINTS = []\n",
    "chk_width = 150\n",
    "chk_height = 10\n",
    "CHECKPOINTS.append(pygame.Rect(150, 50, chk_width, chk_height + 100)) # 0\n",
    "CHECKPOINTS.append(pygame.Rect(550, 50, chk_height + 50, chk_width)) # 1\n",
    "CHECKPOINTS.append(pygame.Rect(950 - chk_height - 100, 250, chk_height, chk_width)) # 2\n",
    "CHECKPOINTS.append(pygame.Rect(850 - chk_width, 650 - chk_height - 100, chk_width, chk_height)) # 3\n",
    "CHECKPOINTS.append(pygame.Rect(150, 500, chk_height, chk_width)) # 4\n",
    "CHECKPOINTS.append(pygame.Rect(50 + 100, 200, chk_height, chk_width)) # 5\n",
    "\n",
    "# Starting position and angle for the car on the new track\n",
    "START_POS_X, START_POS_Y = 100, 200 # Start position from previous fix\n",
    "START_ANGLE = 0\n",
    "\n",
    "# --- Helper function for line segment intersection (Unchanged) ---\n",
    "def line_intersection(p1, p2, p3, p4):\n",
    "    x1, y1 = p1; x2, y2 = p2; x3, y3 = p3; x4, y4 = p4\n",
    "    den = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n",
    "    if den == 0: return None\n",
    "    t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / den\n",
    "    u = -((x1 - x2) * (y1 - y3) - (y1 - y2) * (x1 - x3)) / den\n",
    "    if 0 <= t <= 1 and 0 <= u <= 1:\n",
    "        return (x1 + t * (x2 - x1), y1 + t * (y2 - y1))\n",
    "    return None\n",
    "\n",
    "print(\"Complex track, obstacles, and checkpoints defined.\")\n",
    "print(f\"Start Position: ({START_POS_X}, {START_POS_Y})\")\n",
    "if PYGAME_INITIALIZED:\n",
    "    print(f\"Number of checkpoints: {len(CHECKPOINTS)}\")\n",
    "    print(f\"Number of obstacles: {len(OBSTACLES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 3: Car Environment Class (Updated Reward)\n",
    "import shapely.geometry # Optional, requires pip install shapely\n",
    "\n",
    "class CarEnv:\n",
    "    def __init__(self, render_mode='human'): # render_mode is now mostly for evaluation\n",
    "        self.render_mode = render_mode\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        # Initialize screen only if rendering is explicitly requested for this instance\n",
    "        if self.render_mode == 'human' and PYGAME_INITIALIZED:\n",
    "            try:\n",
    "                self.screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "                pygame.display.set_caption(\"2D Self-Driving Car Sim - Complex\")\n",
    "                self.clock = pygame.time.Clock()\n",
    "            except pygame.error as e:\n",
    "                print(f\"Error creating Pygame window in CarEnv: {e}\")\n",
    "                self.screen = None; self.clock = None\n",
    "                # Don't force mode change, evaluation might still want 'human'\n",
    "                # if self.render_mode == 'human': self.render_mode = 'none'\n",
    "\n",
    "        # --- Define track and obstacle segments FIRST ---\n",
    "        self.track_segments = []\n",
    "        self.obstacle_segments = []\n",
    "        self.all_collision_segments = []\n",
    "        self.outer_polygon = None # For optional Shapely check\n",
    "        self.inner_polygon = None # For optional Shapely check\n",
    "\n",
    "        if 'OUTER_TRACK_POINTS' in globals() and 'INNER_TRACK_POINTS' in globals():\n",
    "            for i in range(len(OUTER_TRACK_POINTS)):\n",
    "                p1 = OUTER_TRACK_POINTS[i]; p2 = OUTER_TRACK_POINTS[(i + 1) % len(OUTER_TRACK_POINTS)]\n",
    "                self.track_segments.append((p1, p2))\n",
    "            for i in range(len(INNER_TRACK_POINTS)):\n",
    "                p1 = INNER_TRACK_POINTS[i]; p2 = INNER_TRACK_POINTS[(i + 1) % len(INNER_TRACK_POINTS)]\n",
    "                self.track_segments.append((p1, p2))\n",
    "            self.all_collision_segments.extend(self.track_segments)\n",
    "            try: # Optional Shapely polygons\n",
    "                self.outer_polygon = shapely.geometry.Polygon(OUTER_TRACK_POINTS)\n",
    "                self.inner_polygon = shapely.geometry.Polygon(INNER_TRACK_POINTS)\n",
    "            except Exception as e: pass # Ignore if shapely not installed or points invalid\n",
    "        else: print(\"Error: Track points not found.\")\n",
    "\n",
    "        if 'OBSTACLES' in globals():\n",
    "            self.obstacles_rects = OBSTACLES\n",
    "            for obs_rect in self.obstacles_rects:\n",
    "                tl, tr, bl, br = obs_rect.topleft, obs_rect.topright, obs_rect.bottomleft, obs_rect.bottomright\n",
    "                segments = [(tl, tr), (tr, br), (br, bl), (bl, tl)]\n",
    "                self.obstacle_segments.extend(segments)\n",
    "            self.all_collision_segments.extend(self.obstacle_segments)\n",
    "        else: self.obstacles_rects = []\n",
    "\n",
    "        self.car_rect = None\n",
    "        self.last_action = None\n",
    "        self.reset()\n",
    "\n",
    "        self.checkpoints_passed = set()\n",
    "        self.last_checkpoint_index = -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.car_x, self.car_y = START_POS_X, START_POS_Y\n",
    "        self.car_angle = START_ANGLE; self.car_speed = 0.0\n",
    "        if self.car_rect is None: self.car_rect = pygame.Rect(0, 0, CAR_WIDTH, CAR_HEIGHT)\n",
    "        self.car_rect.center = (self.car_x, self.car_y)\n",
    "        self.steps_taken = 0; self.total_reward = 0\n",
    "        self.done = False; self.last_action = None\n",
    "        self.checkpoints_passed = set(); self.last_checkpoint_index = -1\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        sensor_readings = self._cast_rays()\n",
    "        normalized_sensors = [dist / SENSOR_RANGE for dist in sensor_readings]\n",
    "        normalized_speed = self.car_speed / MAX_SPEED if MAX_SPEED != 0 else 0\n",
    "        return np.array(normalized_sensors + [normalized_speed], dtype=np.float32)\n",
    "\n",
    "    def _cast_rays(self):\n",
    "        distances = []\n",
    "        car_center_x, car_center_y = self.car_x, self.car_y\n",
    "        angle_step = 180.0 / (NUM_SENSORS - 1) if NUM_SENSORS > 1 else 0\n",
    "        base_relative_angle = -90\n",
    "        for i in range(NUM_SENSORS):\n",
    "            relative_angle = base_relative_angle + (i * angle_step)\n",
    "            ray_angle_rad = math.radians(self.car_angle + relative_angle)\n",
    "            end_x = car_center_x + math.cos(ray_angle_rad) * SENSOR_RANGE\n",
    "            end_y = car_center_y + math.sin(ray_angle_rad) * SENSOR_RANGE\n",
    "            ray_segment = ((car_center_x, car_center_y), (end_x, end_y))\n",
    "            min_dist = SENSOR_RANGE\n",
    "            if hasattr(self, 'all_collision_segments'):\n",
    "                for segment in self.all_collision_segments:\n",
    "                    intersection_point = line_intersection(ray_segment[0], ray_segment[1], segment[0], segment[1])\n",
    "                    if intersection_point:\n",
    "                        dist = math.dist((car_center_x, car_center_y), intersection_point)\n",
    "                        if dist < min_dist: min_dist = dist\n",
    "            distances.append(min_dist)\n",
    "        return distances\n",
    "\n",
    "    def _update_physics(self, action):\n",
    "        steer = 0; acceleration = 0\n",
    "        if action == 0: acceleration = ACCELERATION\n",
    "        elif action == 1: acceleration = -BRAKE_DECEL\n",
    "        elif action == 2: steer = -STEERING_ANGLE\n",
    "        elif action == 3: steer = STEERING_ANGLE\n",
    "\n",
    "        if self.car_speed > 0: self.car_speed -= FRICTION\n",
    "        if abs(self.car_speed) < FRICTION: self.car_speed = 0\n",
    "        self.car_speed += acceleration\n",
    "        self.car_speed = max(MIN_SPEED, min(MAX_SPEED, self.car_speed))\n",
    "\n",
    "        if abs(self.car_speed) > 0.05: self.car_angle += steer\n",
    "        self.car_angle %= 360\n",
    "\n",
    "        angle_rad = math.radians(self.car_angle)\n",
    "        self.car_x += math.cos(angle_rad) * self.car_speed\n",
    "        self.car_y += math.sin(angle_rad) * self.car_speed\n",
    "        if self.car_rect is not None: self.car_rect.center = (self.car_x, self.car_y)\n",
    "\n",
    "    def _check_collision(self):\n",
    "        if self.car_rect is None: return None\n",
    "        center_x, center_y = self.car_rect.center\n",
    "        angle_rad = math.radians(self.car_angle)\n",
    "        cos_a, sin_a = math.cos(angle_rad), math.sin(angle_rad)\n",
    "        hw, hh = CAR_WIDTH / 2, CAR_HEIGHT / 2\n",
    "        relative_corners = [(-hw, -hh), (hw, -hh), (-hw, hh), (hw, hh)]\n",
    "        corners = [(center_x+rx*cos_a-ry*sin_a, center_y+rx*sin_a+ry*cos_a) for rx,ry in relative_corners]\n",
    "        car_edges = [(corners[0],corners[1]),(corners[1],corners[3]),(corners[3],corners[2]),(corners[2],corners[0])]\n",
    "\n",
    "        if hasattr(self, 'obstacle_segments'):\n",
    "            for car_edge in car_edges:\n",
    "                for obs_seg in self.obstacle_segments:\n",
    "                    if line_intersection(car_edge[0], car_edge[1], obs_seg[0], obs_seg[1]): return 'obstacle'\n",
    "        if hasattr(self, 'track_segments'):\n",
    "            for car_edge in car_edges:\n",
    "                for track_seg in self.track_segments:\n",
    "                    if line_intersection(car_edge[0], car_edge[1], track_seg[0], track_seg[1]): return 'track'\n",
    "        # Optional Shapely check\n",
    "        if self.outer_polygon and self.inner_polygon:\n",
    "             car_center_point = shapely.geometry.Point(self.car_x, self.car_y)\n",
    "             if not self.outer_polygon.contains(car_center_point) or self.inner_polygon.contains(car_center_point): return 'track'\n",
    "        return None\n",
    "\n",
    "    def _calculate_reward(self, collision_type, action):\n",
    "        reward = 0; self.done = False\n",
    "        if collision_type == 'obstacle': reward = -150; self.done = True; return reward\n",
    "        elif collision_type == 'track': reward = -100; self.done = True; return reward\n",
    "\n",
    "        checkpoint_reward = 0\n",
    "        current_checkpoint_index = -1\n",
    "        car_center_tuple = (self.car_x, self.car_y)\n",
    "        num_checkpoints = len(CHECKPOINTS) if 'CHECKPOINTS' in globals() else 0\n",
    "        if num_checkpoints > 0:\n",
    "            for i, chkpt_rect in enumerate(CHECKPOINTS):\n",
    "                 if isinstance(chkpt_rect, pygame.Rect) and chkpt_rect.collidepoint(car_center_tuple):\n",
    "                    current_checkpoint_index = i; break\n",
    "            if current_checkpoint_index != -1:\n",
    "                expected_next_checkpoint = (self.last_checkpoint_index + 1) % num_checkpoints\n",
    "                if current_checkpoint_index == expected_next_checkpoint:\n",
    "                    if current_checkpoint_index not in self.checkpoints_passed:\n",
    "                        checkpoint_reward = 60\n",
    "                        self.checkpoints_passed.add(current_checkpoint_index)\n",
    "                        self.last_checkpoint_index = current_checkpoint_index\n",
    "                        if len(self.checkpoints_passed) == num_checkpoints:\n",
    "                            reward += 250; self.checkpoints_passed = set(); self.last_checkpoint_index = -1\n",
    "                            print(f\"Lap Completed! Bonus: +250\") # Keep this print for feedback\n",
    "\n",
    "        speed_reward = self.car_speed * 0.1\n",
    "\n",
    "        # --- Reward Shaping: Proximity Penalty (TUNED) ---\n",
    "        proximity_penalty = 0\n",
    "        sensor_state = self._get_state()[:-1]\n",
    "        min_sensor_dist = min(sensor_state) * SENSOR_RANGE\n",
    "        # ***********************************************************\n",
    "        # ***** ADJUSTED PROXIMITY PENALTY *****\n",
    "        # ***********************************************************\n",
    "        # Penalize if closer than ~40% car length, less harsh magnitude\n",
    "        prox_threshold = CAR_HEIGHT * 0.4\n",
    "        if min_sensor_dist < prox_threshold:\n",
    "             proximity_penalty = -0.8 * (1 - (min_sensor_dist / prox_threshold))**2 # Reduced magnitude from -1.5 to -0.8\n",
    "        # ***********************************************************\n",
    "\n",
    "        time_penalty = -0.1 # Keep encouraging efficiency\n",
    "        reward += checkpoint_reward + speed_reward + proximity_penalty + time_penalty\n",
    "        self.last_action = action\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done: return self._get_state(), 0, self.done, {}\n",
    "        self._update_physics(action)\n",
    "        collision_type = self._check_collision()\n",
    "        reward = self._calculate_reward(collision_type, action)\n",
    "        self.total_reward += reward\n",
    "        next_state = self._get_state()\n",
    "        self.steps_taken += 1\n",
    "        info = {'collision': collision_type}\n",
    "        if not self.done and self.steps_taken >= MAX_STEPS_PER_EPISODE:\n",
    "            self.done = True; reward -= 20\n",
    "        return next_state, reward, self.done, info\n",
    "\n",
    "    def render(self): # Only called during evaluation now\n",
    "        if self.render_mode != 'human' or not PYGAME_INITIALIZED or self.screen is None: return True\n",
    "        try:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT: self.close(); return False\n",
    "        except pygame.error as e: print(f\"Pygame event error: {e}\"); self.close(); return False\n",
    "\n",
    "        self.screen.fill(GRASS_COLOR)\n",
    "        if 'OUTER_TRACK_POINTS' in globals() and 'INNER_TRACK_POINTS' in globals():\n",
    "            pygame.draw.polygon(self.screen, TRACK_COLOR, OUTER_TRACK_POINTS)\n",
    "            pygame.draw.polygon(self.screen, GRASS_COLOR, INNER_TRACK_POINTS)\n",
    "        if hasattr(self, 'obstacles_rects'):\n",
    "            for obs_rect in self.obstacles_rects: pygame.draw.rect(self.screen, OBSTACLE_COLOR, obs_rect)\n",
    "        if 'CHECKPOINTS' in globals() and isinstance(CHECKPOINTS, (list, tuple)):\n",
    "            for i, chkpt_rect in enumerate(CHECKPOINTS):\n",
    "                 if isinstance(chkpt_rect, pygame.Rect):\n",
    "                     s = pygame.Surface((chkpt_rect.width, chkpt_rect.height), pygame.SRCALPHA); s.fill(CHECKPOINT_COLOR)\n",
    "                     self.screen.blit(s, (chkpt_rect.x, chkpt_rect.y))\n",
    "        if self.car_rect is not None:\n",
    "            car_surf = pygame.Surface((CAR_HEIGHT, CAR_WIDTH), pygame.SRCALPHA); car_surf.fill(CAR_COLOR)\n",
    "            pygame.draw.rect(car_surf, (255,255,255), (CAR_HEIGHT*0.7, CAR_WIDTH*0.2, CAR_HEIGHT*0.3, CAR_WIDTH*0.6))\n",
    "            rot_car = pygame.transform.rotate(car_surf, -self.car_angle)\n",
    "            n_rect = rot_car.get_rect(center=(self.car_x, self.car_y)); self.screen.blit(rot_car, n_rect.topleft)\n",
    "\n",
    "        sensor_hit_points = []\n",
    "        car_center_x, car_center_y = self.car_x, self.car_y\n",
    "        angle_step = 180.0 / (NUM_SENSORS - 1) if NUM_SENSORS > 1 else 0; base_relative_angle = -90\n",
    "        if hasattr(self, 'all_collision_segments'):\n",
    "            for i in range(NUM_SENSORS):\n",
    "                relative_angle = base_relative_angle + (i * angle_step); ray_angle_rad = math.radians(self.car_angle + relative_angle)\n",
    "                end_x_far = car_center_x + math.cos(ray_angle_rad) * SENSOR_RANGE; end_y_far = car_center_y + math.sin(ray_angle_rad) * SENSOR_RANGE\n",
    "                ray_segment = ((car_center_x, car_center_y), (end_x_far, end_y_far)); min_dist = SENSOR_RANGE; closest_hit = None\n",
    "                for segment in self.all_collision_segments:\n",
    "                    intersection_point = line_intersection(ray_segment[0], ray_segment[1], segment[0], segment[1])\n",
    "                    if intersection_point: dist = math.dist((car_center_x, car_center_y), intersection_point);\n",
    "                    if intersection_point and dist < min_dist: min_dist = dist; closest_hit = intersection_point\n",
    "                end_x = car_center_x + math.cos(ray_angle_rad) * min_dist; end_y = car_center_y + math.sin(ray_angle_rad) * min_dist\n",
    "                sensor_hit_points.append(((end_x, end_y), min_dist < SENSOR_RANGE * 0.999))\n",
    "        for i, (hit_point, did_hit) in enumerate(sensor_hit_points):\n",
    "            pygame.draw.line(self.screen, SENSOR_COLOR, (car_center_x, car_center_y), hit_point, 1)\n",
    "            if did_hit: pygame.draw.circle(self.screen, (255, 255, 0), (int(hit_point[0]), int(hit_point[1])), 3)\n",
    "\n",
    "        if FONT:\n",
    "            num_chkpts_total = len(CHECKPOINTS) if 'CHECKPOINTS' in globals() else 'N/A'\n",
    "            info_txt = f\"Speed:{self.car_speed:.1f}|Steps:{self.steps_taken}|Ep Reward:{self.total_reward:.1f}\"\n",
    "            chkpt_txt = f\"Checkpoints:{len(self.checkpoints_passed)}/{num_chkpts_total}|Last:{self.last_checkpoint_index}\"\n",
    "            s1 = FONT.render(info_txt, True, (255,255,255), (0,0,0)); self.screen.blit(s1, (10, 10))\n",
    "            s2 = FONT.render(chkpt_txt, True, (255,255,255), (0,0,0)); self.screen.blit(s2, (10, 35))\n",
    "\n",
    "        try: pygame.display.flip()\n",
    "        except pygame.error as e: print(f\"Pygame flip error: {e}\"); self.close(); return False\n",
    "        if self.clock: self.clock.tick(60)\n",
    "        return True\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            try: pygame.display.quit()\n",
    "            except pygame.error as e: print(f\"Error closing Pygame display: {e}\")\n",
    "            self.screen = None; print(\"Pygame display quit.\")\n",
    "\n",
    "# --- Test Block (Optional - No changes needed, but won't render unless mode='human') ---\n",
    "# [ ... Keep the test block from the previous response if desired ... ]\n",
    "# [ ... It will only show output if you manually change render_mode='human' when creating env ... ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde909c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 4: DQN Agent (Updated Network - Dueling DQN)\n",
    "\n",
    "# --- Q-Network Definition (Dueling Architecture) ---\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=HIDDEN_SIZE):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Shared layers\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # ***********************************************************\n",
    "        # ***** DUELING DQN ARCHITECTURE *****\n",
    "        # ***********************************************************\n",
    "        # State Value stream\n",
    "        self.fc_value = nn.Linear(hidden_size, hidden_size // 2) # Intermediate layer for value\n",
    "        self.out_value = nn.Linear(hidden_size // 2, 1)          # Output: single value V(s)\n",
    "\n",
    "        # Action Advantage stream\n",
    "        self.fc_advantage = nn.Linear(hidden_size, hidden_size // 2) # Intermediate layer for advantage\n",
    "        self.out_advantage = nn.Linear(hidden_size // 2, action_size) # Output: advantage A(s, a) for each action\n",
    "        # ***********************************************************\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Shared layers forward pass\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Dueling streams forward pass\n",
    "        value_hidden = F.relu(self.fc_value(x))\n",
    "        value = self.out_value(value_hidden) # V(s)\n",
    "\n",
    "        advantage_hidden = F.relu(self.fc_advantage(x))\n",
    "        advantage = self.out_advantage(advantage_hidden) # A(s, a)\n",
    "\n",
    "        # Combine value and advantage streams to get Q-values\n",
    "        # Q(s, a) = V(s) + (A(s, a) - mean(A(s, a')))\n",
    "        # This improves stability by ensuring advantages sum roughly to zero\n",
    "        q_values = value + (advantage - advantage.mean(dim=-1, keepdim=True))\n",
    "\n",
    "        return q_values # Output final Q-values for each action\n",
    "\n",
    "# --- Replay Buffer (Unchanged) ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=BUFFER_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size=BATCH_SIZE):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    def __len__(self): return len(self.buffer)\n",
    "\n",
    "# --- DQN Agent (Class structure unchanged, uses new QNetwork) ---\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size=STATE_SIZE, action_size=ACTION_SIZE, hidden_size=HIDDEN_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon_start=EPSILON_START, epsilon_end=EPSILON_END, epsilon_decay=EPSILON_DECAY, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, target_update_freq=TARGET_UPDATE_FREQ):\n",
    "        self.state_size = state_size; self.action_size = action_size\n",
    "        self.gamma = gamma; self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end; self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size; self.target_update_freq = target_update_freq\n",
    "\n",
    "        # Uses the new Dueling QNetwork definition\n",
    "        self.policy_net = QNetwork(state_size, action_size, hidden_size).to(device)\n",
    "        self.target_net = QNetwork(state_size, action_size, hidden_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict()); self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon: return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        self.policy_net.eval()\n",
    "        with torch.no_grad(): action_values = self.policy_net(state)\n",
    "        self.policy_net.train()\n",
    "        return torch.argmax(action_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size: return None\n",
    "        experiences = self.memory.sample(self.batch_size); batch = list(zip(*experiences))\n",
    "        states = torch.FloatTensor(np.array(batch[0])).to(device)\n",
    "        actions = torch.LongTensor(np.array(batch[1])).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(np.array(batch[2])).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(batch[3])).to(device)\n",
    "        dones = torch.FloatTensor(np.array(batch[4])).unsqueeze(1).to(device)\n",
    "\n",
    "        # Double DQN update: Use policy_net to select best action for next state,\n",
    "        # but use target_net to evaluate that action's Q-value.\n",
    "        next_actions_policy = self.policy_net(next_states).max(1)[1].unsqueeze(1) # Get actions from policy net\n",
    "        next_q_values_target = self.target_net(next_states).gather(1, next_actions_policy) # Get Q-values from target net using those actions\n",
    "\n",
    "        # Original DQN target calculation (commented out for reference)\n",
    "        # next_q_values_target = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        target_q_values = rewards + (self.gamma * next_q_values_target * (1 - dones))\n",
    "        current_q_values = self.policy_net(states).gather(1, actions)\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad(); loss.backward()\n",
    "        # Optional: torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0) # Clip norm instead of value\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Epsilon decay after each training step\n",
    "        if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decay\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "         self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def load(self, filename):\n",
    "        try:\n",
    "            self.policy_net.load_state_dict(torch.load(filename, map_location=device))\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            print(f\"Model weights loaded from {filename} to {device}\")\n",
    "        except Exception as e: print(f\"Error loading model weights from {filename}: {e}\")\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.policy_net.state_dict(), filename)\n",
    "\n",
    "# --- Test the Agent (optional) ---\n",
    "agent = DQNAgent() # Uses updated defaults\n",
    "print(\"Dueling DQN Agent created.\")\n",
    "dummy_state = np.random.rand(STATE_SIZE).astype(np.float32)\n",
    "action = agent.act(dummy_state)\n",
    "print(f\"Agent chose action: {action} (Epsilon: {agent.epsilon:.5f})\") # Show more precision for slow decay\n",
    "agent.remember(dummy_state, action, 1.0, dummy_state, False)\n",
    "print(f\"Memory size: {len(agent.memory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 5: Training Loop (Updated - No Rendering)\n",
    "\n",
    "# plot_rewards function remains the same as in the previous response\n",
    "# Ensure it's defined here or in a previous cell\n",
    "def plot_rewards(episode_rewards_list, losses_list, avg_rewards_list, filename=\"training_progress.png\", save_dir=PLOT_SAVE_DIR):\n",
    "    if not episode_rewards_list: print(\"No data to plot.\"); return\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1) # Rewards\n",
    "    plt.plot(episode_rewards_list, label='Episode Reward', alpha=0.7)\n",
    "    plt.plot(avg_rewards_list, label=f'Avg Reward (Last 100)', linewidth=2, color='orange')\n",
    "    plt.xlabel(\"Episode\"); plt.ylabel(\"Reward\"); plt.title(\"Episode Rewards\")\n",
    "    plt.legend(); plt.grid(True)\n",
    "    # Adjust ylim dynamically based on rewards, avoid extreme lows if any\n",
    "    min_r = np.percentile(episode_rewards_list, 5) if len(episode_rewards_list)>1 else min(episode_rewards_list) - 50\n",
    "    max_r = np.percentile(episode_rewards_list, 95) if len(episode_rewards_list)>1 else max(episode_rewards_list) + 50\n",
    "    plt.ylim(bottom=min_r - abs(min_r*0.1), top=max_r + abs(max_r*0.1))\n",
    "\n",
    "    plt.subplot(1, 2, 2) # Loss\n",
    "    valid_losses = [l for l in losses_list if l is not None and not math.isnan(l)]\n",
    "    if valid_losses:\n",
    "         num_losses = len(valid_losses)\n",
    "         plt.plot(valid_losses, label='Batch Loss', alpha=0.3, color='lightblue')\n",
    "         avg_window = min(max(num_losses // 20, 100), 1000) # Adjust window size logic\n",
    "         if num_losses >= avg_window:\n",
    "             loss_avg = np.convolve(valid_losses, np.ones(avg_window)/avg_window, mode='valid')\n",
    "             loss_avg_x = np.arange(avg_window - 1, num_losses)\n",
    "             plt.plot(loss_avg_x, loss_avg, label=f'Avg Loss ({avg_window} batches)', linewidth=2, color='red')\n",
    "         plt.xlabel(\"Training Batch Step\"); plt.ylabel(\"Loss (MSE)\"); plt.title(\"Training Loss\")\n",
    "         plt.yscale('log'); plt.legend(); plt.grid(True)\n",
    "    else: plt.text(0.5, 0.5, 'No loss data yet', ha='center', va='center')\n",
    "    plt.suptitle(f\"Training Progress ({len(episode_rewards_list)} Episodes)\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    try: plt.savefig(save_path); print(f\"Plot saved to {save_path}\")\n",
    "    except Exception as e: print(f\"Error saving plot: {e}\")\n",
    "    plt.close()\n",
    "\n",
    "# --- Main Training Function ---\n",
    "def train():\n",
    "    # render_during_training is effectively False now due to RENDER_EVERY_N_EPISODES = 0\n",
    "    # Create env without human mode unless specifically needed elsewhere\n",
    "    env = CarEnv(render_mode='none') # Explicitly set to 'none'\n",
    "    agent = DQNAgent() # Uses updated defaults and Dueling QNetwork\n",
    "\n",
    "    all_episode_rewards = []; all_average_rewards = []\n",
    "    all_losses = []; best_avg_reward = -float('inf')\n",
    "    global_step_counter = 0\n",
    "\n",
    "    print(f\"Starting training for {NUM_EPISODES} episodes...\")\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for e in range(NUM_EPISODES):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0; episode_losses = []\n",
    "        start_time_episode = time.time()\n",
    "\n",
    "        # No rendering check needed here anymore\n",
    "\n",
    "        for step in range(MAX_STEPS_PER_EPISODE):\n",
    "            # No rendering call here anymore\n",
    "\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            loss = agent.replay() # Contains epsilon decay now\n",
    "            if loss is not None:\n",
    "                all_losses.append(loss); episode_losses.append(loss)\n",
    "                global_step_counter += 1\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        # --- End of Episode ---\n",
    "        all_episode_rewards.append(episode_reward)\n",
    "        avg_reward = np.mean(all_episode_rewards[-100:])\n",
    "        all_average_rewards.append(avg_reward)\n",
    "\n",
    "        if e % agent.target_update_freq == 0: agent.update_target_network()\n",
    "\n",
    "        episode_duration = time.time() - start_time_episode\n",
    "        avg_ep_loss = np.mean(episode_losses) if episode_losses else 0\n",
    "        print(f\"Ep {e+1}/{NUM_EPISODES} | Reward: {episode_reward:.2f} | Avg Reward(100): {avg_reward:.2f} | Epsilon: {agent.epsilon:.5f} | Steps: {step+1} | Avg Loss: {avg_ep_loss:.4f} | Duration: {episode_duration:.1f}s\")\n",
    "\n",
    "        is_best = avg_reward > best_avg_reward\n",
    "        if is_best:\n",
    "             best_avg_reward = avg_reward\n",
    "             print(f\"  New best average reward: {avg_reward:.2f}. Saving best model...\")\n",
    "             agent.save(os.path.join(MODEL_SAVE_DIR, \"dqn_car_best.pth\"))\n",
    "        if (e + 1) % MODEL_SAVE_FREQ == 0:\n",
    "             agent.save(os.path.join(MODEL_SAVE_DIR, f\"dqn_car_episode_{e+1}.pth\"))\n",
    "\n",
    "        if (e + 1) % PLOT_UPDATE_FREQ == 0:\n",
    "             plot_rewards(all_episode_rewards, all_losses, all_average_rewards, filename=f\"training_progress_ep{e+1}.png\")\n",
    "\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(f\"Training finished in {total_training_time / 60:.2f} minutes.\")\n",
    "    # env.close() # No need to close if screen was never created\n",
    "\n",
    "    agent.save(os.path.join(MODEL_SAVE_DIR, \"dqn_car_final.pth\"))\n",
    "    plot_rewards(all_episode_rewards, all_losses, all_average_rewards, filename=\"training_progress_final.png\")\n",
    "    return all_episode_rewards, all_losses, all_average_rewards\n",
    "\n",
    "# --- Start Training ---\n",
    "start_time_total = time.time()\n",
    "try:\n",
    "    print(f\"Models will be saved in: {MODEL_SAVE_DIR}\")\n",
    "    print(f\"Plots will be saved in: {PLOT_SAVE_DIR}\")\n",
    "    rewards_history, losses_history, avg_rewards_history = train()\n",
    "except NameError as ne: print(f\"NameError: {ne}. Make sure all previous cells are executed.\")\n",
    "except Exception as ex: print(f\"An error occurred during training: {ex}\"); import traceback; traceback.print_exc()\n",
    "finally:\n",
    "    end_time_total = time.time()\n",
    "    print(f\"Total script time: {(end_time_total - start_time_total)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 6: Evaluation and Visualization (Unchanged)\n",
    "# Purpose: Load a saved model and watch the agent drive WITH visualization.\n",
    "\n",
    "def evaluate_agent(model_path, num_episodes=5):\n",
    "    global PYGAME_INITIALIZED, FONT\n",
    "    if not PYGAME_INITIALIZED:\n",
    "         try:\n",
    "             pygame.init(); pygame.font.init(); FONT = pygame.font.SysFont(\"Arial\", 20)\n",
    "             print(\"Pygame re-initialized for evaluation.\"); PYGAME_INITIALIZED = True\n",
    "         except pygame.error as e: print(f\"Pygame init failed for eval: {e}\"); return\n",
    "\n",
    "    try:\n",
    "        # IMPORTANT: Create env in 'human' mode for evaluation!\n",
    "        env = CarEnv(render_mode='human')\n",
    "    except Exception as e: print(f\"Error creating CarEnv for evaluation: {e}\"); return\n",
    "\n",
    "    # Ensure agent parameters match the loaded model's training parameters\n",
    "    # (STATE_SIZE, ACTION_SIZE, HIDDEN_SIZE should match Cell 1 settings used for training that model)\n",
    "    agent = DQNAgent(epsilon_start=0.0, epsilon_end=0.0) # No exploration\n",
    "    agent.load(model_path)\n",
    "    if hasattr(agent, 'policy_net'): agent.policy_net.eval() # Set model to evaluation mode\n",
    "\n",
    "    print(f\"\\n--- Evaluating model: {model_path} ---\")\n",
    "    print(f\"Running {num_episodes} episodes with rendering...\")\n",
    "    all_episode_rewards = []; all_episode_steps = []\n",
    "\n",
    "    for e in range(num_episodes):\n",
    "        try:\n",
    "            state = env.reset()\n",
    "        except Exception as e:\n",
    "            print(f\"Error resetting env in evaluation: {e}\"); break # Stop if env fails\n",
    "        episode_reward = 0; done = False; steps = 0\n",
    "        print(f\"\\n--- Starting Evaluation Episode {e+1} ---\")\n",
    "\n",
    "        while not done:\n",
    "            if not env.render(): # Render and check for quit event\n",
    "                print(\"Pygame window closed by user during evaluation.\")\n",
    "                env.close(); return # Stop evaluation\n",
    "\n",
    "            action = agent.act(state)\n",
    "            try:\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during env.step in evaluation: {e}\"); done=True; info={'collision':'error'} # End episode on error\n",
    "                 next_state = state # Keep state same on error\n",
    "                 reward = -500 # Penalize error state\n",
    "\n",
    "            state = next_state; episode_reward += reward; steps += 1\n",
    "            # time.sleep(0.01) # Optional slowdown\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {e+1} finished: Steps={steps}, Reward={episode_reward:.2f}, Collision={info.get('collision')}\")\n",
    "                all_episode_rewards.append(episode_reward); all_episode_steps.append(steps)\n",
    "                time.sleep(1) # Pause\n",
    "\n",
    "    env.close() # Close window after all episodes\n",
    "    print(\"\\n--- Evaluation Summary ---\")\n",
    "    if all_episode_rewards:\n",
    "        print(f\"Episodes run: {len(all_episode_rewards)}\")\n",
    "        print(f\"Avg Reward: {np.mean(all_episode_rewards):.2f} | Min: {np.min(all_episode_rewards):.2f} | Max: {np.max(all_episode_rewards):.2f}\")\n",
    "        print(f\"Avg Steps: {np.mean(all_episode_steps):.1f}\")\n",
    "    else: print(\"No episodes completed.\")\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "# Choose the model from the NEW save directory\n",
    "model_to_evaluate = os.path.join(MODEL_SAVE_DIR, \"dqn_car_best.pth\") # Or _final.pth or specific episode\n",
    "# model_to_evaluate = os.path.join(MODEL_SAVE_DIR, \"dqn_car_final.pth\")\n",
    "\n",
    "if os.path.exists(model_to_evaluate):\n",
    "    evaluate_agent(model_to_evaluate, num_episodes=5)\n",
    "else:\n",
    "    print(f\"Model file not found: {model_to_evaluate}\")\n",
    "    print(f\"Ensure training ran and saved models to '{MODEL_SAVE_DIR}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
